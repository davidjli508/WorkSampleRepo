{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats701 Homework 10  \n",
    "Written By: David Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulted with Inson Zeng about general approach tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary Loading of packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Tensorflow logo\n",
    "Spent about 1 hour on this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Drawing the tensorflow logo with a constant 5 by 4 by 3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 3)\n",
      "[[[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 1.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# As following recommendation, creating a numpy array first with 5 by 4 by 3 dimensions with 0 indexing\n",
    "# 5 is the shape for height of the logo from top to bottom, \n",
    "# 4 is the shape for length in axis direction of the T projection from left to right, \n",
    "# 3 is the shape for length in axis direction of the F projection from right to left\n",
    "\n",
    "# Initialize the dimension and lengths\n",
    "logonumpy = np.zeros((5, 4, 3))\n",
    "\n",
    "# Draw the top cross of T\n",
    "logonumpy[0,:,2] = 1\n",
    "\n",
    "# Draw the top of F\n",
    "logonumpy[0,3,:] = 1\n",
    "\n",
    "# Draw the vertical of T\n",
    "logonumpy[:,2,2] = 1\n",
    "\n",
    "# Draw the middle cube for F\n",
    "logonumpy[2,2,1] = 1\n",
    "\n",
    "# Converting to a constant tensor with shape 5 by 4 by 3\n",
    "# Using the tensorflow function convert_to_tensor, which is:\n",
    "#   This function converts Python objects of various types to Tensor objects. \n",
    "#   It accepts Tensor objects, numpy arrays, Python lists, and Python scalars.\n",
    "logotensor = tf.convert_to_tensor(logonumpy)\n",
    "\n",
    "# Testing\n",
    "sess = tf.Session()\n",
    "# Shape\n",
    "print(logotensor.shape) # Expecting (5,4,3)\n",
    "# Printing for verification, each instance is the top-down view of a height layer with the (0,3,2) corner as the bottom right\n",
    "print(logotensor.eval(session=sess))\n",
    "\n",
    "sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building and training simple models  \n",
    "Spent about 8 hours on this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to build the logistic regression tensorflow model\n",
    "\n",
    "# Training data\n",
    "logis_xtrain = np.load(\"/home/david/Desktop/School/Stats701/Homework10/logistic_data/logistic_xtrain.npy\")\n",
    "logis_ytrain = np.load(\"/home/david/Desktop/School/Stats701/Homework10/logistic_data/logistic_ytrain.npy\")\n",
    "\n",
    "# Test Data\n",
    "logis_xtest = np.load(\"/home/david/Desktop/School/Stats701/Homework10/logistic_data/logistic_xtest.npy\")\n",
    "logis_ytest = np.load(\"/home/david/Desktop/School/Stats701/Homework10/logistic_data/logistic_ytest.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Logistic regression with a negative log-likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As discussed in lecture, each Tensorflow model involves two steps: Building the computational graph and running it.\n",
    "\n",
    "# First, we build the computational graph. This step involves initializing \n",
    "#  Constants, where their value never changes\n",
    "#  Placeholders, where their value is obtained from elsewhere in our program (in this case, we are loading data)\n",
    "#  Variables, where their value can be tuned and adjusted accordingly.\n",
    "\n",
    "# There are no constants within the log-likelihood function\n",
    "\n",
    "# Consider these placeholder inputs, to be initially given:\n",
    "ytrue = tf.placeholder(tf.float32, [None, 1]) # utilizes a 1-column of the actual data response values\n",
    "x = tf.placeholder(tf.float32, [None, 6]) # utilizes a 6-column of unspecified rows, representing the data predictors\n",
    "\n",
    "# Consider these variable parameters, which we might change as we tune the model:\n",
    "W = tf.Variable(tf.zeros([6, 1]), dtype = tf.float32) # must be 6 x 1, to correctly matrix multiply by the 6-column predictors\n",
    "b = tf.Variable([0], dtype=tf.float32) # the bias parameter\n",
    "\n",
    "# Model predictions based on given predictors\n",
    "ypredicted = tf.matmul(x, W) + b # the predicted y\n",
    "probpredicted = 1/(1+tf.exp(-1*ypredicted)) # the predicted probability estimate\n",
    "\n",
    "# representing the derived negative log-likelihood loss function\n",
    "logisticloss = tf.reduce_sum((ytrue * tf.log(probpredicted)) + ((1-ytrue)*tf.log(1-probpredicted)))\n",
    "neglogisticloss = logisticloss * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Estimating parameters in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W and b parameters, pre-optimization:\n",
      "[array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32), array([0.], dtype=float32)]\n",
      "Training data loss, pre-optimization:\n",
      "1386.2922\n",
      "##############################################\n",
      "W and b parameters, post-optimization:\n",
      "[array([[1.0921205 ],\n",
      "       [0.87264544],\n",
      "       [2.3463395 ],\n",
      "       [3.1160364 ],\n",
      "       [5.0267167 ],\n",
      "       [8.290963  ]], dtype=float32), array([-0.21865678], dtype=float32)]\n",
      "Training data loss, post-optimization:\n",
      "722.3639\n"
     ]
    }
   ],
   "source": [
    "# To complete our model, we lastly need to estimate reasonable W and b parameters.\n",
    "# This will involve gradient descent, which we approach from black-box approach from lecture notes\n",
    "\n",
    "# First we may need parameters to use in the gradient descent optimizer. \n",
    "# Let's set the learning rate to 0.01 and the number of steps to be 500.\n",
    "\n",
    "# Gradient descent procedure\n",
    "minimizer = tf.train.GradientDescentOptimizer(0.01).minimize(neglogisticloss)\n",
    "\n",
    "# Begin running a smaller session to estimate the W and b parameters\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# We plan on comparing training data loss before and after gradient descent optimization, and the variable values\n",
    "# Before Optimization\n",
    "\n",
    "print(\"W and b parameters, pre-optimization:\")\n",
    "print(sess.run([W, b]))\n",
    "print(\"Training data loss, pre-optimization:\")\n",
    "print(sess.run(neglogisticloss, feed_dict={x: logis_xtrain, ytrue: logis_ytrain}))\n",
    "print(\"##############################################\")\n",
    "\n",
    "# Running gradient descent procedure (the optimization)\n",
    "for i in range(500):\n",
    "    sess.run(minimizer, feed_dict={x: logis_xtrain, ytrue: logis_ytrain})\n",
    "\n",
    "# After Optimization\n",
    "\n",
    "print(\"W and b parameters, post-optimization:\")\n",
    "print(sess.run([W, b]))\n",
    "print(\"Training data loss, post-optimization:\")\n",
    "print(sess.run(neglogisticloss, feed_dict={x: logis_xtrain, ytrue: logis_ytrain}))\n",
    "\n",
    "# W is approximately [1.0921, 0.8726, 2.3463, 3.116, 5.026, 8.2909] and b is approximately [-0.21865]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Evaluating logistic regression on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W and b parameters, post-optimization:\n",
      "[array([[1.0921205 ],\n",
      "       [0.87264544],\n",
      "       [2.3463395 ],\n",
      "       [3.1160364 ],\n",
      "       [5.0267167 ],\n",
      "       [8.290963  ]], dtype=float32), array([-0.21865678], dtype=float32)]\n",
      "Test data loss, post-optimization:\n",
      "180.82965\n"
     ]
    }
   ],
   "source": [
    "# The session still contains the optimized W and b parameters, let's keep using these\n",
    "\n",
    "print(\"W and b parameters, post-optimization:\")\n",
    "print(sess.run([W, b]))\n",
    "print(\"Test data loss, post-optimization:\")\n",
    "print(sess.run(neglogisticloss, feed_dict={x: logis_xtest, ytrue: logis_ytest}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Evaluating the estimated logistic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared error between estimated W, b  parameters and truth W, b  parameters is: 0.85399145\n"
     ]
    }
   ],
   "source": [
    "# Specs tell us the truth values of the W and b parameters, so let's see how off we were repesented as a squared error sum\n",
    "W_truth = tf.constant([1.0, 1.0, 2.0, 3.0, 5.0, 8.0], shape=[6, 1], dtype=tf.float32) # float32 for consistency\n",
    "b_truth = tf.constant([-1.0], dtype=tf.float32) # float32 for consistency\n",
    "squarederror = tf.reduce_sum((W_truth - W) * (W_truth - W)) + tf.reduce_sum((b_truth - b) * (b_truth - b))\n",
    "\n",
    "with sess.as_default():\n",
    "    print(\"Squared error between estimated W, b  parameters and truth W, b  parameters is:\", squarederror.eval())\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to build the normally distributed classification tensorflow model\n",
    "\n",
    "# Training data\n",
    "norm_xtrain = np.load(\"/home/david/Desktop/School/Stats701/Homework10/normal_data/normal_xtrain.npy\")\n",
    "norm_ytrain = np.load(\"/home/david/Desktop/School/Stats701/Homework10/normal_data/normal_ytrain.npy\")\n",
    "\n",
    "# Test Data\n",
    "norm_xtest = np.load(\"/home/david/Desktop/School/Stats701/Homework10/normal_data/normal_xtest.npy\")\n",
    "norm_ytest = np.load(\"/home/david/Desktop/School/Stats701/Homework10/normal_data/normal_ytest.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Classification of normally distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9810104], dtype=float32), array([0.5114171], dtype=float32)]\n",
      "[array([0.00807609], dtype=float32), array([1.0041919], dtype=float32)]\n",
      "[array([3.0174267], dtype=float32), array([1.4122144], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# As discussed in lecture, each Tensorflow model involves two steps: Building the computational graph and running it.\n",
    "\n",
    "# First, we build the computational graph. This step involves initializing \n",
    "#  Constants, where their value never changes\n",
    "#  Placeholders, where their value is obtained from elsewhere in our program (in this case, we are loading data)\n",
    "#  Variables, where their value can be tuned and adjusted accordingly.\n",
    "\n",
    "# There are no applicable tensor constants\n",
    "\n",
    "# Consider these placeholder inputs, to be initially given:\n",
    "ytrue = tf.placeholder(tf.float32, [None, 3]) # utilizes a 3-column of the actual data response values, in one-hot encoding style\n",
    "x = tf.placeholder(tf.float32) # utilizes a 1-column of data predictors\n",
    "probs = tf.placeholder(tf.float32, shape = [3])\n",
    "\n",
    "# Consider these variable parameters, which we might change as we tune the model:\n",
    "mu0 = tf.Variable(tf.zeros([1]), dtype = tf.float32)\n",
    "var0 = tf.Variable(tf.ones([1]), dtype = tf.float32)\n",
    "mu1 = tf.Variable(tf.zeros([1]), dtype = tf.float32)\n",
    "var1 = tf.Variable(tf.ones([1]), dtype = tf.float32)\n",
    "mu2 = tf.Variable(tf.zeros([1]), dtype = tf.float32)\n",
    "var2 = tf.Variable(tf.ones([1]), dtype = tf.float32)\n",
    "\n",
    "# For code readability\n",
    "pie = np.pi # pi\n",
    "\n",
    "# Establish the densities for each normally distributed class\n",
    "density0 = ((1/(tf.sqrt(2*pie*var0))) * tf.exp((-1*((x-mu0)**2))/(2*var0)))\n",
    "density1 = ((1/(tf.sqrt(2*pie*var1))) * tf.exp((-1*((x-mu1)**2))/(2*var1)))\n",
    "density2 = ((1/(tf.sqrt(2*pie*var2))) * tf.exp((-1*((x-mu2)**2))/(2*var2)))\n",
    "\n",
    "# Represent the cross-entropy for one observation, as defined in specs\n",
    "cross_entropy=-1 * (tf.reduce_sum(ytrue*tf.log(tf.transpose([density0,density1,density2]))))\n",
    "\n",
    "# Establish the Adagrad Optimizer, with learning rate 0.01 and number of steps as 5000\n",
    "adaminimizer=tf.train.AdagradOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "# Create a session for running the normal classification models\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Tuning the means and variances\n",
    "for i in range(5000):\n",
    "    sess.run(adaminimizer, feed_dict = {x: norm_xtrain, ytrue: norm_ytrain})\n",
    "\n",
    "# Estimates of class means and variances\n",
    "# Class 0, mu and variance\n",
    "print(sess.run([mu0, var0]))\n",
    "# Class 1, mu and variance\n",
    "print(sess.run([mu1, var1]))\n",
    "# Class 2, mu and variance\n",
    "print(sess.run([mu2, var2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: Evaluating loss on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676.53253\n"
     ]
    }
   ],
   "source": [
    "# Run the cross-entropy function on the test data\n",
    "print(sess.run(cross_entropy, {x: norm_xtest, ytrue: norm_ytest}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: Evaluating parameter estimation on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True parameters for each class:\n",
    "\n",
    "$$\\mu_0 =âˆ’1, \\sigma_0^2 =0.5$$ \n",
    "$$\\mu_1 = 0, \\sigma_1^2 = 1$$\n",
    "$$\\mu_2 = 3, \\sigma_2^2 = 1.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared error between estimated mu and var parameters vs. their truth counterparts is: 0.008583753\n"
     ]
    }
   ],
   "source": [
    "mu0_truth = tf.constant([-1.0], dtype=tf.float32) # float32 for consistency\n",
    "mu1_truth = tf.constant([0.0], dtype=tf.float32) # float32 for consistency\n",
    "mu2_truth = tf.constant([3.0], dtype=tf.float32) # float32 for consistency\n",
    "var0_truth = tf.constant([0.5], dtype=tf.float32) # float32 for consistency\n",
    "var1_truth = tf.constant([1.0], dtype=tf.float32) # float32 for consistency\n",
    "var2_truth = tf.constant([1.5], dtype=tf.float32) # float32 for consistency\n",
    "\n",
    "squarederror = tf.reduce_sum(((mu0_truth - sess.run([mu0])[0])**2) + ((var0_truth - sess.run([var0])[0])**2) +\n",
    "                             ((mu1_truth - sess.run([mu1])[0])**2) + ((var1_truth - sess.run([var1])[0])**2) +\n",
    "                             ((mu2_truth - sess.run([mu2])[0])**2) + ((var2_truth - sess.run([var2])[0])**2))\n",
    "\n",
    "with sess.as_default():\n",
    "   print(\"Squared error between estimated mu and var parameters vs. their truth counterparts is:\", squarederror.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8: Evaluating classification error on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "# Refer to the tensorflow normal distribution\n",
    "normdists = tf.distributions.Normal(loc=[mu0,mu1,mu2], scale = [tf.sqrt(var0), tf.sqrt(var1),tf.sqrt(var2)])\n",
    "# Note that I'm using variance, and the tensorflow normdistribution uses standard deviation\n",
    "normdists2 = tf.distributions.Normal(loc=[mu0_truth,mu1_truth,mu2_truth], scale = [tf.sqrt(var0_truth), tf.sqrt(var1_truth),tf.sqrt(var2_truth)])\n",
    "\n",
    "# Calculate what proportion of predictions were correct\n",
    "prop_pred_correct = tf.equal(tf.argmax(tf.map_fn(lambda i: tf.log(normdists.prob(i)),x)[:,:,0],1), tf.argmax(ytrue, 1))\n",
    "\n",
    "# Average the proportions as requested\n",
    "classaccuracy = tf.reduce_mean(tf.cast(prop_pred_correct, tf.float64)) # using float64 for more precision\n",
    "\n",
    "# Print the average classification error proportion of estimated model on test data\n",
    "print(sess.run(classaccuracy, {x: norm_xtest, ytrue: norm_ytest}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# For Number 4 later\n",
    "\n",
    "# Get the probabilities of belonging to each distribution\n",
    "probs = normdists2.prob([x, x, x])\n",
    "# Get the index aka group of max likelihood\n",
    "prediction = tf.argmax(probs)[0]\n",
    "# Testing\n",
    "print(sess.run(prediction, feed_dict={x: 4.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current session as a Tensorflow Model, for use in Question 4\n",
    "tf.saved_model.simple_save(session = sess,\n",
    "            export_dir = \"/home/david/Desktop/School/Stats701/Homework10/davidjli_normal_trained/\",\n",
    "            inputs={\"x\": x},\n",
    "            outputs = {\"prediction\":prediction})\n",
    "\n",
    "# Close the Session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Building a Complicated Model\n",
    "Spent about 2 hours on this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've picked to do the Linear Model for a binary classification problem. URL: https://www.tensorflow.org/tutorials/wide\n",
    "\n",
    "The tutorial includes instructions to download the data, but I will simply include the data in my submission for simplicity. The datafiles are adult.data and adult.test, which are included in the included lm_data folder within my submission. The command executed to download the data is below, assuming data_download.py is available for use (which the code was given in the tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run data_download --data_dir lm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpqcd485lw', '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_task_type': 'worker', '_global_id_in_cluster': 0, '_save_summary_steps': 100, '_service': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbfe8743cc0>, '_tf_random_seed': None, '_num_worker_replicas': 1, '_task_id': 0, '_is_chief': True, '_master': ''}\n",
      "Parsing /home/david/Desktop/School/Stats701/Homework10/lm_data/adult.data\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpqcd485lw/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 27.725887\n",
      "INFO:tensorflow:global_step/sec: 282.374\n",
      "INFO:tensorflow:step = 101, loss = 498.91696 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 720.843\n",
      "INFO:tensorflow:step = 201, loss = 75.049644 (0.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 604.018\n",
      "INFO:tensorflow:step = 301, loss = 8.565516 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 608.117\n",
      "INFO:tensorflow:step = 401, loss = 148.40434 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 703.103\n",
      "INFO:tensorflow:step = 501, loss = 15.55579 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 553.126\n",
      "INFO:tensorflow:step = 601, loss = 32.969666 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.506\n",
      "INFO:tensorflow:step = 701, loss = 97.54366 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 765.405\n",
      "INFO:tensorflow:step = 801, loss = 11.330231 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 596.876\n",
      "INFO:tensorflow:step = 901, loss = 22.63941 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 736.283\n",
      "INFO:tensorflow:step = 1001, loss = 79.41222 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 780.519\n",
      "INFO:tensorflow:step = 1101, loss = 311.52606 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.34\n",
      "INFO:tensorflow:step = 1201, loss = 16.872854 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 715.262\n",
      "INFO:tensorflow:step = 1301, loss = 131.5946 (0.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 639.211\n",
      "INFO:tensorflow:step = 1401, loss = 98.27154 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 627.742\n",
      "INFO:tensorflow:step = 1501, loss = 16.697002 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.436\n",
      "INFO:tensorflow:step = 1601, loss = 20.911531 (0.160 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1629 into /tmp/tmpqcd485lw/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.124332875.\n",
      "Parsing /home/david/Desktop/School/Stats701/Homework10/lm_data/adult.test\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-23-21:45:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqcd485lw/model.ckpt-1629\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-23-21:45:06\n",
      "INFO:tensorflow:Saving dict for global step 1629: accuracy = 0.8307229, accuracy_baseline = 0.76377374, auc = 0.8769814, auc_precision_recall = 0.64815927, average_loss = 2.6838877, global_step = 1629, label/mean = 0.23622628, loss = 107.23037, prediction/mean = 0.2438061\n",
      "accuracy: 0.8307229\n",
      "accuracy_baseline: 0.76377374\n",
      "auc: 0.8769814\n",
      "auc_precision_recall: 0.64815927\n",
      "average_loss: 2.6838877\n",
      "global_step: 1629\n",
      "label/mean: 0.23622628\n",
      "loss: 107.23037\n",
      "prediction/mean: 0.2438061\n"
     ]
    }
   ],
   "source": [
    "## TUTORIAL CODE ###\n",
    "\n",
    "# Census Income Dataset metadata\n",
    "CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                        [0], [0], [0], [''], ['']]\n",
    "CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "\n",
    "# Number of rows in each set of data\n",
    "num_examples = {'train': 32561, 'validation': 16281}\n",
    "\n",
    "# Creating the Input Builder function to specify inputs for the tf.estimator model\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size, buf_size):\n",
    "    \"\"\"\n",
    "    Generate an input function for the Estimator\n",
    "    Returns x, y \n",
    "    where x is a mapping of feature column names to Tensors with that feature's data\n",
    "    and y is a Tensor containing labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Binary Classification, for labeling 1 if the income is over 50K and 0 if not\n",
    "    def parse_csv(value):\n",
    "        print('Parsing', data_file)\n",
    "        columns = tf.decode_csv(value, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(CSV_COLUMNS, columns))\n",
    "        labels = features.pop('income_bracket')\n",
    "        return features, tf.equal(labels, '>50K')\n",
    "\n",
    "    # using the dataset API to do extraction from the input files\n",
    "    # continuous columns -> Tensor, categorical data -> SparseTensor\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "    \n",
    "    # Optionally, shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buf_size)\n",
    "\n",
    "    # parse_csv on the dataset\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "\n",
    "    # Handling the epochs and shuffling\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "## adjusting some feature columns to be used in the model\n",
    "\n",
    "# defining for categorical features with specified set of values\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'education', [\n",
    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "# hashbucket for categorical features with no specified set of values\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'occupation', hash_bucket_size=1000)\n",
    "\n",
    "# repeat for all other categorical features\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'marital_status', [\n",
    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'workclass', [\n",
    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'occupation', hash_bucket_size=1000)\n",
    "\n",
    "# continuous features\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "# continuous features -> categorical with bucketization\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "# there are interactions between some predictors, these will handle the interactions\n",
    "education_x_occupation = tf.feature_column.crossed_column(\n",
    "    [education, 'occupation'], hash_bucket_size=1000)\n",
    "\n",
    "age_buckets_x_education_x_occupation = tf.feature_column.crossed_column(\n",
    "    [age_buckets, education, 'occupation'], hash_bucket_size=1000)\n",
    "\n",
    "# Use Tensorflow's LinearClassifier model with the features we've collected\n",
    "base_columns = [education, marital_status, occupation, age,\n",
    "                capital_gain, hours_per_week, workclass]\n",
    "crossed_columns = [education_x_occupation,\n",
    "                   age_buckets_x_education_x_occupation]\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "model = tf.estimator.LinearClassifier(model_dir=model_dir, \n",
    "                                      feature_columns=base_columns+crossed_columns)\n",
    "\n",
    "# parameters for the training data\n",
    "train_data = \"/home/david/Desktop/School/Stats701/Homework10/lm_data/adult.data\"\n",
    "num_epochs = 2\n",
    "batch_size = 40\n",
    "do_shuffle = True\n",
    "\n",
    "# training our model\n",
    "model.train(input_fn=lambda: input_fn(train_data, num_epochs, do_shuffle, \n",
    "                                      batch_size, num_examples['train']))\n",
    "\n",
    "# parameters for the test data\n",
    "test_data = \"/home/david/Desktop/School/Stats701/Homework10/lm_data/adult.test\"\n",
    "num_epochs = 2\n",
    "batch_size = 40\n",
    "do_shuffle = True\n",
    "\n",
    "# testing our model\n",
    "results = model.evaluate(input_fn=lambda: input_fn(\n",
    "    test_data, num_epochs, do_shuffle, batch_size, num_examples['validation']))\n",
    "\n",
    "# print some metrics on how the model ran on the test data\n",
    "for key in sorted(results):\n",
    "    print('%s: %s' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LinearClassifier Model shows that the accuracy on the test data was around 0.8333, pretty good! I used 2 epochs for the training set and 2 epochs for the test set with batch sizes of 40 each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Running Models on Google Cloud Platform\n",
    "Spent about 6 hours on this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: CNN on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done all the tutorial steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Revisiting normally-distributed classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Upload normally-distributed classifier to GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Create a version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Create .json of single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: Do a single prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "aznpianoguy@plucky-point-202003:~$ gcloud ml-engine predict --model davidjli_stat701_hw10_normal --version davidjli_hw10 --json-instances davidjli.instance.json\n",
    "PREDICTION\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My prediction model with x=4 in the json file says the likely cluster is cluster 2, which is mean 3 and variance 1.5. Makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
