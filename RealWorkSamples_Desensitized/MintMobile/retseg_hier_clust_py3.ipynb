{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5561cc1-f84c-485c-b3fd-bc51c40f2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector as snow\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#from collections import Counter\n",
    "\n",
    "import umap\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac7a7b-7384-4c94-84ae-6bfa14378983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS & FUNCTIONS Section\n",
    "\n",
    "# CONSTANTS\n",
    "ORIGIN_COLS = ['ACTIVATING_SALE_GROUP_NAME_grouped',\n",
    "'PROMO_GROUPED',\n",
    "'GSMA_OPERATING_SYSTEM_grouped',\n",
    "'ZERO_USAGE_LAST_30D_FLAG',\n",
    "'SUB_AUTO_RENEWAL_FLAG',\n",
    "'SUB_CREDIT_CARD_FLAG',\n",
    "'PLAN_CYCLE_NUM_grouped',\n",
    "'FAILED_PAYMENT_GROUPED',\n",
    "'MEMBER_OF_ACTIVE_FAMILY_FLAG',\n",
    "#'ACS_HP_PROP', \n",
    "#'ACS_NOT_HP_ASIAN_ALONE_PROP',\n",
    "#'ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP',\n",
    "#'ACS_NOT_HP_WHITE_ALONE_PROP',\n",
    "#'ACS_PROP_WORKERS_OVER_16',\n",
    "#'ACS_AGE_MEDIAN',\n",
    "#'ACS_INCOME_MEDIAN',\n",
    "#'ACS_APPROX_COMMUTE_MEDIAN',\n",
    "#'NO_SCHOOL_PROP',\n",
    "#'ANY_DEGREE_PROP',\n",
    "#'SINGLE_MOM_PROP',\n",
    "#'NEVER_MARRIED_PROP',\n",
    "#'HH_WO_INT_ACCESS_PROP',\n",
    "#'OCCUP_HOUS_UNIT_WO_CAR_PROP',\n",
    "'UPGRADE_DOWNGRADE_DATA_FLAG', \n",
    "'UPGRADE_DOWNGRADE_DURATION_FLAG',\n",
    "'SUB_ESIM_FLAG',\n",
    "'PORTIN_FLAG',\n",
    "#'HAD_ISSUES_PORTING_IN',\n",
    "'EVER_LOGGED_INTO_APP_FLAG',\n",
    "'YEAR_RELEASED_grouped',\n",
    "'LTE_BAND_71',\n",
    "#'contacted_care_last7d', \n",
    "'contacted_care_last30d',\n",
    "'SERVICE_ISSUE_NOTE_FLAG', \n",
    "#'SIM_REPLACEMENT_NOTE_FLAG',\n",
    "#'PAYMENT_NOTE_FLAG',\n",
    "'TENURE_MONTHS',\n",
    "'EXPECTED_CLV_PS',\n",
    "'PROMO_FLAG',\n",
    "'PORTIN_ISSUE_DESC',\n",
    "'L1_CLUST'\n",
    "]\n",
    "\n",
    "# Demographic Info Only\n",
    "ML_COLS_L1 = ['ACS_HP_PROP',\n",
    "       'ACS_NOT_HP_ASIAN_ALONE_PROP',\n",
    "       'ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP',\n",
    "       'ACS_NOT_HP_WHITE_ALONE_PROP', \n",
    "       'ACS_PROP_WORKERS_OVER_16',\n",
    "       'ACS_AGE_MEDIAN', \n",
    "       'ACS_INCOME_MEDIAN',\n",
    "       'ACS_APPROX_COMMUTE_MEDIAN',\n",
    "        'NO_SCHOOL_PROP',\n",
    "        'ANY_DEGREE_PROP',\n",
    "        'SINGLE_MOM_PROP',\n",
    "        'NEVER_MARRIED_PROP',\n",
    "        'HH_WO_INT_ACCESS_PROP',\n",
    "        'OCCUP_HOUS_UNIT_WO_CAR_PROP']\n",
    "\n",
    "# All Columns\n",
    "ML_COLS_L3 = ['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm',\n",
    "    'ACTIVATING_SALE_GROUP_NAME_grouped_National Retail', \n",
    "    'PROMO_GROUPED_Deflation', \n",
    "    #'PROMO_GROUPED_Device bundle', \n",
    "    'PROMO_GROUPED_No Promo',\n",
    "    'GSMA_OPERATING_SYSTEM_grouped_iOS',\n",
    "    'PLAN_CYCLE_NUM_grouped', \n",
    "    'FAILED_PAYMENT_GROUPED',\n",
    "    'MEMBER_OF_ACTIVE_FAMILY_FLAG', \n",
    "    #'ACS_HP_PROP',\n",
    "    #'ACS_NOT_HP_ASIAN_ALONE_PROP',\n",
    "    #'ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP',\n",
    "    #'ACS_NOT_HP_WHITE_ALONE_PROP', \n",
    "    #'ACS_PROP_WORKERS_OVER_16',\n",
    "    #'ACS_AGE_MEDIAN', \n",
    "    #'ACS_INCOME_MEDIAN', \n",
    "    #'ACS_APPROX_COMMUTE_MEDIAN', \n",
    "    #'NO_SCHOOL_PROP',\n",
    "    #'ANY_DEGREE_PROP',\n",
    "    #'SINGLE_MOM_PROP',\n",
    "    #'NEVER_MARRIED_PROP',\n",
    "    #'HH_WO_INT_ACCESS_PROP',\n",
    "    #'OCCUP_HOUS_UNIT_WO_CAR_PROP',\n",
    "    'UPGRADE_DOWNGRADE_DATA_FLAG', \n",
    "    'UPGRADE_DOWNGRADE_DURATION_FLAG',\n",
    "    'SUB_ESIM_FLAG', \n",
    "    #'HAD_ISSUES_PORTING_IN', \n",
    "    'EVER_LOGGED_INTO_APP_FLAG',\n",
    "    'LTE_BAND_71',\n",
    "    #'contacted_care_last7d', \n",
    "    'contacted_care_last30d',\n",
    "    'SERVICE_ISSUE_NOTE_FLAG',\n",
    "    #'SIM_REPLACEMENT_NOTE_FLAG', \n",
    "    #'PAYMENT_NOTE_FLAG',\n",
    "    'EXPECTED_CLV_PS',\n",
    "    'PORTIN_ISSUE_DESC_NoIssues',\n",
    "    'PORTIN_ISSUE_DESC_NonPortin',\n",
    "    'L1_CLUST_0',\n",
    "    #'L1_CLUST_1',\n",
    "    'L1_CLUST_2',\n",
    "    'L1_CLUST_3',\n",
    "    'L1_CLUST_4',\n",
    "    'L1_CLUST_5',\n",
    "    'L1_CLUST_6'\n",
    "]\n",
    "\n",
    "# Columns to Encode\n",
    "ENCODE_COLS = ['ACTIVATING_SALE_GROUP_NAME_grouped', 'PROMO_GROUPED', 'GSMA_OPERATING_SYSTEM_grouped', 'PORTIN_ISSUE_DESC', 'L1_CLUST']\n",
    "\n",
    "# Columns to Stratify On\n",
    "STRATIFY_COLS = ['BLK_GRP', 'PROMO_FLAG', 'PROMO_GROUPED', 'LTE_BAND_71', 'ACTIVATING_SALE_GROUP_NAME']\n",
    "\n",
    "# FUNCTIONS\n",
    "def categorize_sale_group(column):\n",
    "    if 'Direct EComm' in str(column):\n",
    "        return 'Direct EComm'\n",
    "    if 'National Retail' in str(column):\n",
    "        return 'National Retail'\n",
    "    if 'Campus SIMs' in str(column):\n",
    "        return 'Campus SIMs'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def categorize_device_year(column):\n",
    "    try:\n",
    "        # Attempt to convert the column to an integer\n",
    "        column = int(float(column))\n",
    "    except (ValueError, TypeError):\n",
    "        # If conversion fails, set column to 0\n",
    "        column = 0\n",
    "\n",
    "    # Now perform the categorization\n",
    "    if column == 2024:\n",
    "        return 2024\n",
    "    elif column == 2023:\n",
    "        return 2023\n",
    "    elif column == 2022:\n",
    "        return 2022\n",
    "    elif column == 2021:\n",
    "        return 2021\n",
    "    elif column == 2020:\n",
    "        return 2020\n",
    "    elif column == 0:  # accounting for majority of devices being Galaxy A53 5G\n",
    "        return 2022\n",
    "    else:\n",
    "        return 2019\n",
    "\n",
    "def categorize_os(column):\n",
    "    if 'Android' in str(column):\n",
    "        return 'Android'\n",
    "    if 'KaiOS' in str(column):\n",
    "        return 'Other'\n",
    "    if 'iOS' in str(column):\n",
    "        return 'iOS'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def categorize_plan_cycle_num(column):\n",
    "    if column <= 3:\n",
    "        return column\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Fit/Transforms a Scalar object on provided df, and exports a scalar object\n",
    "def min_max_scale_export(df, columns_to_scale, scalar_path):\n",
    "    \"\"\"\n",
    "    Applies Min-Max scaling to specified columns of a DataFrame to a range between 0 and 1, \n",
    "    ensuring all values are non-negative.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - columns_to_scale: List of column names to apply Min-Max Scaling.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the specified columns scaled to the range between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Adjust feature_range to (0, 1) for scaling between 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Apply MinMaxScaler to the specified columns\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    # Save the scaler to a file using pickle\n",
    "    with open(scalar_path, 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    \n",
    "    return df[columns_to_scale]\n",
    "\n",
    "# Applies a provided scalar object to the df\n",
    "def min_max_scale_import(df, columns_to_scale, scalar_path):\n",
    "    \"\"\"\n",
    "    Loads a saved scaler and applies it to specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - columns_to_scale: List of column names to apply the saved scaler.\n",
    "    - scaler_path: Path to the saved scaler object.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the specified columns scaled using the saved scaler.\n",
    "    \"\"\"\n",
    "    # Load the scaler from the file using pickle\n",
    "    with open(scalar_path, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    \n",
    "    # Apply the loaded scaler to the specified columns\n",
    "    df[columns_to_scale] = scaler.transform(df[columns_to_scale])\n",
    "    \n",
    "    return df[columns_to_scale]\n",
    "\n",
    "# Fit/Transforms a UMAP Reducer object on provided df, and exports a UMAP Reducer object\n",
    "def umap_export(df, columns_to_reduce, n_components, umap_path):\n",
    "    \"\"\"\n",
    "    Perform UMAP dimensionality reduction on specified columns of a DataFrame and save the transformer.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to perform UMAP on.\n",
    "    columns_to_reduce (list): The subset of columns to reduce the dimensions of.\n",
    "    n_components (int): The number of components to reduce to.\n",
    "    umap_path (str): The path to save the UMAP transformer in pickle form.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with reduced dimensions.\n",
    "    \"\"\"\n",
    "    # Select the specified columns to reduce\n",
    "    data_to_reduce = df[columns_to_reduce]\n",
    "\n",
    "    # Initialize UMAP with the desired number of components\n",
    "    reducer = umap.UMAP(n_components=n_components)\n",
    "\n",
    "    # Fit the UMAP model and transform the data\n",
    "    embedding = reducer.fit_transform(data_to_reduce)\n",
    "\n",
    "    # Save the trained UMAP model using pickle\n",
    "    with open(umap_path, 'wb') as file:\n",
    "        pickle.dump(reducer, file)\n",
    "\n",
    "    # Convert the result to a DataFrame for easier handling\n",
    "    embedding_df = pd.DataFrame(embedding, columns=[f'UMAP{i+1}' for i in range(n_components)])\n",
    "\n",
    "    return embedding_df\n",
    "\n",
    "# Applies a provided UMAP Reducer object to the df\n",
    "def umap_import(df, columns_to_reduce, n_components, umap_path):\n",
    "    \"\"\"\n",
    "    Perform UMAP dimensionality reduction on specified columns of a DataFrame and save the transformer.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to perform UMAP on.\n",
    "    columns_to_reduce (list): The subset of columns to reduce the dimensions of.\n",
    "    n_components (int): The number of components to reduce to.\n",
    "    umap_path (str): The path to save the UMAP transformer in pickle form.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with reduced dimensions.\n",
    "    \"\"\"\n",
    "    # Select the specified columns to reduce\n",
    "    data_to_reduce = df[columns_to_reduce]\n",
    "\n",
    "    # Initialize UMAP with the desired number of components\n",
    "    reducer = umap.UMAP(n_components=n_components)\n",
    "\n",
    "    # Fit the UMAP model and transform the data\n",
    "    embedding = reducer.fit_transform(data_to_reduce)\n",
    "\n",
    "    # Save the trained UMAP model using pickle\n",
    "    with open(umap_path, 'wb') as file:\n",
    "        pickle.dump(reducer, file)\n",
    "\n",
    "    # Convert the result to a DataFrame for easier handling\n",
    "    embedding_df = pd.DataFrame(embedding, columns=[f'UMAP{i+1}' for i in range(n_components)])\n",
    "\n",
    "    return embedding_df\n",
    "\n",
    "# Define Pre-processing Steps\n",
    "def hierPreprocess(df):\n",
    "    ## Impute ACS Statistic Measures where NA\n",
    "    # Age\n",
    "    df['ACS_AGE_MEDIAN'] = df['TOTAL_POP_MEDIAN_AGE'].fillna(df['ACS_APPROX_AGE_MEDIAN'])\n",
    "    df['ACS_AGE_MEDIAN'] = df['ACS_AGE_MEDIAN'].fillna(df['ACS_AGE_MEDIAN'].median())\n",
    "    # Income\n",
    "    df['ACS_INCOME_MEDIAN'] = df['ACS_APPROX_INCOME_MEDIAN'].fillna(df['ACS_APPROX_INCOME_MEDIAN'].median())\n",
    "    # Commute\n",
    "    df['ACS_APPROX_COMMUTE_MEDIAN'] = df['ACS_APPROX_COMMUTE_MEDIAN'].fillna(df['ACS_APPROX_COMMUTE_MEDIAN'].median())\n",
    "    # Ethnicities\n",
    "    df['ACS_HP_PROP'] = df['ACS_HP_PROP'].fillna(df['ACS_HP_PROP'].median())\n",
    "    df['ACS_NOT_HP_ASIAN_ALONE_PROP'] = df['ACS_NOT_HP_ASIAN_ALONE_PROP'].fillna(df['ACS_NOT_HP_ASIAN_ALONE_PROP'].median())\n",
    "    df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'] = df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].fillna(df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].median())\n",
    "    df['ACS_NOT_HP_WHITE_ALONE_PROP'] = df['ACS_NOT_HP_WHITE_ALONE_PROP'].fillna(df['ACS_NOT_HP_WHITE_ALONE_PROP'].median())\n",
    "    # Proportion of Workers\n",
    "    df['ACS_PROP_WORKERS_OVER_16'] = df['ACS_PROP_WORKERS_OVER_16'].fillna(df['ACS_PROP_WORKERS_OVER_16'].median())\n",
    "\n",
    "    # Use Categorization/Bucketing Functions\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped'] = df['ACTIVATING_SALE_GROUP_NAME'].map(categorize_sale_group)\n",
    "    df['YEAR_RELEASED_grouped'] = df['YEAR_RELEASED'].map(categorize_device_year)\n",
    "    df['GSMA_OPERATING_SYSTEM_grouped'] = df['GSMA_OPERATING_SYSTEM'].map(categorize_os)\n",
    "    df['PLAN_CYCLE_NUM_grouped'] = df['PLAN_CYCLE_NUM'].map(categorize_plan_cycle_num)\n",
    "    df['contacted_care_last7d'] = np.where(df['CNT_NOTES_LAST_7D']>0,1,0)\n",
    "    df['contacted_care_last30d'] = np.where(df['CNT_NOTES_LAST_30D']>0,1,0)\n",
    "    \n",
    "    # Function to determine PORTIN_ISSUE_DESC\n",
    "    def determine_issue_desc(row):\n",
    "        if row['PORTIN_FLAG'] == 1 and row['HAD_ISSUES_PORTING_IN'] == 1:\n",
    "            return 'HadIssues'\n",
    "        elif row['PORTIN_FLAG'] == 1 and row['HAD_ISSUES_PORTING_IN'] == 0:\n",
    "            return 'NoIssues'\n",
    "        elif row['PORTIN_FLAG'] == 0:\n",
    "            return 'NonPortin'\n",
    "        else:\n",
    "            return 'UnKnown'  # In case there are other combinations\n",
    "    # Apply the function to create the new column\n",
    "    df['PORTIN_ISSUE_DESC'] = df.apply(determine_issue_desc, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def coerceDataTypes_small(df):\n",
    "    print(\"Memory Used Before Coercing: \")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    \n",
    "    #df['VOLTE_USAGE_LAST_7D']=df['VOLTE_USAGE_LAST_7D'].astype('int8')\n",
    "    df['PROMO_GROUPED_Deflation']=df['PROMO_GROUPED_Deflation'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail']=df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail'].astype('int8')\n",
    "    #df['ACS_INCOME_MEDIAN']=df['ACS_INCOME_MEDIAN'].astype('float16')\n",
    "    df['PLAN_CYCLE_NUM_grouped']=df['PLAN_CYCLE_NUM_grouped'].astype('int8')\n",
    "    df['FAILED_PAYMENT_GROUPED']=df['FAILED_PAYMENT_GROUPED'].astype('int8')  \n",
    "    df['EXPECTED_CLV_PS']=df['EXPECTED_CLV_PS'].astype('float16')   \n",
    "    df['MEMBER_OF_ACTIVE_FAMILY_FLAG']=df['MEMBER_OF_ACTIVE_FAMILY_FLAG'].astype('int8')\n",
    "    #df['ACS_HP_PROP']=df['ACS_HP_PROP'].astype('float16')\n",
    "    #df['ACS_NOT_HP_ASIAN_ALONE_PROP']=df['ACS_NOT_HP_ASIAN_ALONE_PROP'].astype('float16')\n",
    "    #df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP']=df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].astype('float16')\n",
    "    #df['ACS_NOT_HP_WHITE_ALONE_PROP']=df['ACS_NOT_HP_WHITE_ALONE_PROP'].astype('float16')\n",
    "    #df['ACS_PROP_WORKERS_OVER_16']=df['ACS_PROP_WORKERS_OVER_16'].astype('float16')\n",
    "    #df['ACS_AGE_MEDIAN']=df['ACS_AGE_MEDIAN'].astype('float16')\n",
    "    #df['ACS_APPROX_COMMUTE_MEDIAN']=df['ACS_APPROX_COMMUTE_MEDIAN'].astype('float16')\n",
    "    df['SUB_ESIM_FLAG']=df['SUB_ESIM_FLAG'].astype('int8')\n",
    "    #df['HAD_ISSUES_PORTING_IN']=df['HAD_ISSUES_PORTING_IN'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NoIssues']=df['PORTIN_ISSUE_DESC_NoIssues'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NonPortin']=df['PORTIN_ISSUE_DESC_NonPortin'].astype('int8')\n",
    "    df['EVER_LOGGED_INTO_APP_FLAG']=df['EVER_LOGGED_INTO_APP_FLAG'].astype('int8')\n",
    "    df['LTE_BAND_71']=df['LTE_BAND_71'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DATA_FLAG']=df['UPGRADE_DOWNGRADE_DATA_FLAG'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DURATION_FLAG']=df['UPGRADE_DOWNGRADE_DURATION_FLAG'].astype('int8')\n",
    "    #df['contacted_care_last7d']=df['contacted_care_last7d'].astype('int8')\n",
    "    df['contacted_care_last30d']=df['contacted_care_last30d'].astype('int8')\n",
    "    df['SERVICE_ISSUE_NOTE_FLAG']=df['SERVICE_ISSUE_NOTE_FLAG'].astype('int8')\n",
    "    #df['SIM_REPLACEMENT_NOTE_FLAG']=df['SIM_REPLACEMENT_NOTE_FLAG'].astype('int8')\n",
    "    #df['PAYMENT_NOTE_FLAG']=df['PAYMENT_NOTE_FLAG'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm']=df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm'].astype('int8')\n",
    "    #df['PROMO_GROUPED_Device bundle']=df['PROMO_GROUPED_Device bundle'].astype('int8')\n",
    "    df['PROMO_GROUPED_No Promo']=df['PROMO_GROUPED_No Promo'].astype('int8')\n",
    "    df['GSMA_OPERATING_SYSTEM_grouped_iOS']=df['GSMA_OPERATING_SYSTEM_grouped_iOS'].astype('int8')\n",
    "    #df['NO_SCHOOL_PROP']=df['NO_SCHOOL_PROP'].astype('float16')\n",
    "    #df['ANY_DEGREE_PROP']=df['ANY_DEGREE_PROP'].astype('float16')\n",
    "    #df['SINGLE_MOM_PROP']=df['SINGLE_MOM_PROP'].astype('float16')\n",
    "    #df['NEVER_MARRIED_PROP']=df['NEVER_MARRIED_PROP'].astype('float16')\n",
    "    #df['HH_WO_INT_ACCESS_PROP']=df['HH_WO_INT_ACCESS_PROP'].astype('float16')\n",
    "    #df['OCCUP_HOUS_UNIT_WO_CAR_PROP']=df['OCCUP_HOUS_UNIT_WO_CAR_PROP'].astype('float16')\n",
    "    df['L1_CLUST_0']=df['L1_CLUST_0'].astype('int8')\n",
    "    #df['L1_CLUST_1']=df['L1_CLUST_1'].astype('int8')\n",
    "    df['L1_CLUST_2']=df['L1_CLUST_2'].astype('int8')\n",
    "    df['L1_CLUST_3']=df['L1_CLUST_3'].astype('int8')\n",
    "    df['L1_CLUST_4']=df['L1_CLUST_4'].astype('int8')\n",
    "    df['L1_CLUST_5']=df['L1_CLUST_5'].astype('int8')\n",
    "    df['L1_CLUST_6']=df['L1_CLUST_6'].astype('int8')\n",
    "\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"Memory Used After Coercing\")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    print(\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def coerceDataTypes_med(df):\n",
    "    print(\"Memory Used Before Coercing: \")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    \n",
    "    #df['VOLTE_USAGE_LAST_7D']=df['VOLTE_USAGE_LAST_7D'].astype('int8')\n",
    "    df['PROMO_GROUPED_Deflation']=df['PROMO_GROUPED_Deflation'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail']=df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail'].astype('int8')\n",
    "    #df['ACS_INCOME_MEDIAN']=df['ACS_INCOME_MEDIAN'].astype('float32')\n",
    "    df['PLAN_CYCLE_NUM_grouped']=df['PLAN_CYCLE_NUM_grouped'].astype('int8')\n",
    "    df['FAILED_PAYMENT_GROUPED']=df['FAILED_PAYMENT_GROUPED'].astype('int8') \n",
    "    df['EXPECTED_CLV_PS']=df['EXPECTED_CLV_PS'].astype('float32')  \n",
    "    df['MEMBER_OF_ACTIVE_FAMILY_FLAG']=df['MEMBER_OF_ACTIVE_FAMILY_FLAG'].astype('int8')\n",
    "    #df['ACS_HP_PROP']=df['ACS_HP_PROP'].astype('float32')\n",
    "    #df['ACS_NOT_HP_ASIAN_ALONE_PROP']=df['ACS_NOT_HP_ASIAN_ALONE_PROP'].astype('float32')\n",
    "    #df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP']=df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].astype('float32')\n",
    "    #df['ACS_NOT_HP_WHITE_ALONE_PROP']=df['ACS_NOT_HP_WHITE_ALONE_PROP'].astype('float32')\n",
    "    #df['ACS_PROP_WORKERS_OVER_16']=df['ACS_PROP_WORKERS_OVER_16'].astype('float32')\n",
    "    #df['ACS_AGE_MEDIAN']=df['ACS_AGE_MEDIAN'].astype('float32')\n",
    "    #df['ACS_APPROX_COMMUTE_MEDIAN']=df['ACS_APPROX_COMMUTE_MEDIAN'].astype('float32')\n",
    "    df['SUB_ESIM_FLAG']=df['SUB_ESIM_FLAG'].astype('int8')\n",
    "    #df['HAD_ISSUES_PORTING_IN']=df['HAD_ISSUES_PORTING_IN'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NoIssues']=df['PORTIN_ISSUE_DESC_NoIssues'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NonPortin']=df['PORTIN_ISSUE_DESC_NonPortin'].astype('int8')\n",
    "    df['EVER_LOGGED_INTO_APP_FLAG']=df['EVER_LOGGED_INTO_APP_FLAG'].astype('int8')\n",
    "    df['LTE_BAND_71']=df['LTE_BAND_71'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DATA_FLAG']=df['UPGRADE_DOWNGRADE_DATA_FLAG'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DURATION_FLAG']=df['UPGRADE_DOWNGRADE_DURATION_FLAG'].astype('int8')\n",
    "    #df['contacted_care_last7d']=df['contacted_care_last7d'].astype('int8')\n",
    "    df['contacted_care_last30d']=df['contacted_care_last30d'].astype('int8')\n",
    "    df['SERVICE_ISSUE_NOTE_FLAG']=df['SERVICE_ISSUE_NOTE_FLAG'].astype('int8')\n",
    "    #df['SIM_REPLACEMENT_NOTE_FLAG']=df['SIM_REPLACEMENT_NOTE_FLAG'].astype('int8')\n",
    "    #df['PAYMENT_NOTE_FLAG']=df['PAYMENT_NOTE_FLAG'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm']=df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm'].astype('int8')\n",
    "    #df['PROMO_GROUPED_Device bundle']=df['PROMO_GROUPED_Device bundle'].astype('int8')\n",
    "    df['PROMO_GROUPED_No Promo']=df['PROMO_GROUPED_No Promo'].astype('int8')\n",
    "    df['GSMA_OPERATING_SYSTEM_grouped_iOS']=df['GSMA_OPERATING_SYSTEM_grouped_iOS'].astype('int8')\n",
    "    #df['NO_SCHOOL_PROP']=df['NO_SCHOOL_PROP'].astype('float32')\n",
    "    #df['ANY_DEGREE_PROP']=df['ANY_DEGREE_PROP'].astype('float32')\n",
    "    #df['SINGLE_MOM_PROP']=df['SINGLE_MOM_PROP'].astype('float32')\n",
    "    #df['NEVER_MARRIED_PROP']=df['NEVER_MARRIED_PROP'].astype('float32')\n",
    "    #df['HH_WO_INT_ACCESS_PROP']=df['HH_WO_INT_ACCESS_PROP'].astype('float32')\n",
    "    #df['OCCUP_HOUS_UNIT_WO_CAR_PROP']=df['OCCUP_HOUS_UNIT_WO_CAR_PROP'].astype('float32')\n",
    "    df['L1_CLUST_0']=df['L1_CLUST_0'].astype('int8')\n",
    "    #df['L1_CLUST_1']=df['L1_CLUST_1'].astype('int8')\n",
    "    df['L1_CLUST_2']=df['L1_CLUST_2'].astype('int8')\n",
    "    df['L1_CLUST_3']=df['L1_CLUST_3'].astype('int8')\n",
    "    df['L1_CLUST_4']=df['L1_CLUST_4'].astype('int8')\n",
    "    df['L1_CLUST_5']=df['L1_CLUST_5'].astype('int8')\n",
    "    df['L1_CLUST_6']=df['L1_CLUST_6'].astype('int8')\n",
    "\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"Memory Used After Coercing\")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    print(\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fixed Sample Ratio\n",
    "DF_SAMPLEPROP = 0.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320368a-d14a-45a5-92d0-d912f8f2ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection Details\n",
    "con = snow.connect(\n",
    "    user=\"DESENSITIZED\",\n",
    "    server=\"DESENSITIZED\",\n",
    "    database=\"DESENSITIZED\",\n",
    "    warehouse=\"DESENSITIZED\",\n",
    "    authenticator=\"externalbrowser\",\n",
    "    account=\"DESENSITIZED\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e23e30-1038-46d6-8774-55fb96222e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor object.\n",
    "cur = con.cursor()\n",
    "\n",
    "jun_stratify_sql = '''\n",
    "DESENSITIZED\n",
    "'''\n",
    "\n",
    "jun_subs_sql = '''\n",
    "DESENSITIZED\n",
    "\n",
    "'''\n",
    "\n",
    "# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "# Grab Dataframes\n",
    "cur.execute(jun_stratify_sql)\n",
    "jun_stratify_df = cur.fetch_pandas_all()\n",
    "\n",
    "cur.execute(jun_subs_sql)\n",
    "jun_subs_df = cur.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529eaf54-3ffe-4393-9e13-9156a75628d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jun_subs_df.memory_usage(deep=True).sum()/1000/1000/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50493af4-65ab-4dc5-925c-2a938b6442f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data\n",
    "jun_subs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e06286-33d2-47f5-bce5-1227374afdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data\n",
    "jun_subs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f459c3-407a-4d53-91cc-20d1a9454cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the Joins to turn them from 1:many to 1:1 to simplify things\n",
    "jun_subs_df.drop_duplicates(subset='SUB_BILLING_ID', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62abf2d-5755-4142-9b76-b8ae7521c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data\n",
    "jun_subs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_stratify_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64df6ea-1379-4669-b07d-9b0b6c75caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Stratified Sample out of the Overlapped Data\n",
    "jun_stratify_sampled_df = jun_stratify_df.groupby(STRATIFY_COLS, group_keys=False).apply(lambda x: x.sample(frac=DF_SAMPLEPROP)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c022ab-ed01-4129-b4e1-144bfb8df7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_stratify_sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc36f5-f2b7-4151-ba3b-6f6eced4ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_stratify_sampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f73731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Stratifying Columns\n",
    "jun_stratify_sampled_df.drop(columns = STRATIFY_COLS, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf52a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_stratify_sampled_df.drop(columns = \"SNAPSHOT_DATE\", axis = 1, inplace = True)\n",
    "jun_stratify_sampled_df.drop(columns = \"ACS_BLOCK_GROUP\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe24c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_stratify_sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_stratify_sampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307f5cb-881d-43ad-a970-d1931425b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Overlap May and Jun, and get the data for May\n",
    "jun_data_df = jun_stratify_sampled_df.merge(jun_subs_df, how = \"left\", on = \"SUB_BILLING_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e38f01-a5cf-4e26-a0a9-5016a1d610c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8e5fd-dcef-47c2-9019-68e56be34ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for Data Consistency\n",
    "#jun_data_df.to_csv(\"jun_data_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ba08b-fe68-49c0-9a57-2a6736ca2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jun_data_df.memory_usage(deep=True).sum()/1000/1000/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NAs\n",
    "# Set the option to display more rows\n",
    "pd.set_option('display.max_rows', 100)\n",
    "# Check the number of missing values in each column\n",
    "na_counts = jun_data_df.isna().sum()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "sorted_na_counts = na_counts.sort_values(ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(sorted_na_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e732b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NAs appropriately\n",
    "jun_data_df['VOLTE_USAGE_LAST_7D'] = jun_data_df['VOLTE_USAGE_LAST_7D'].fillna(0)\n",
    "jun_data_df['ZERO_USAGE_LAST_30D_FLAG'] = jun_data_df['ZERO_USAGE_LAST_30D_FLAG'].fillna(0)\n",
    "jun_data_df['SERVICE_ISSUE_NOTE_FLAG'] = jun_data_df['SERVICE_ISSUE_NOTE_FLAG'].fillna(0)\n",
    "jun_data_df['SIM_REPLACEMENT_NOTE_FLAG'] = jun_data_df['SIM_REPLACEMENT_NOTE_FLAG'].fillna(0)\n",
    "jun_data_df['PAYMENT_NOTE_FLAG'] = jun_data_df['PAYMENT_NOTE_FLAG'].fillna(0)\n",
    "jun_data_df['LTE_BAND_71'] = jun_data_df['LTE_BAND_71'].fillna(1)\n",
    "jun_data_df['HAD_ISSUES_PORTING_IN'] = jun_data_df['HAD_ISSUES_PORTING_IN'].fillna(0)\n",
    "jun_data_df['EXPECTED_CLV_PS'] = jun_data_df['EXPECTED_CLV_PS'].fillna(0)\n",
    "jun_data_df['UPGRADE_DOWNGRADE_DATA_FLAG'] = jun_data_df['UPGRADE_DOWNGRADE_DATA_FLAG'].fillna(0)\n",
    "jun_data_df['CNT_NOTES_LAST_30D'] = jun_data_df['CNT_NOTES_LAST_30D'].fillna(0)\n",
    "jun_data_df['CNT_NOTES_LAST_7D'] = jun_data_df['CNT_NOTES_LAST_7D'].fillna(0)\n",
    "jun_data_df['PLAN_CYCLE_NUM'] = jun_data_df['PLAN_CYCLE_NUM'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NAs\n",
    "# Set the option to display more rows\n",
    "pd.set_option('display.max_rows', 100)\n",
    "# Check the number of missing values in each column\n",
    "na_counts = jun_data_df.isna().sum()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "sorted_na_counts = na_counts.sort_values(ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(sorted_na_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "jun_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ca2f4-f457-4191-8b6c-b7e7e8830747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modelling Process\n",
    "def performHierClustering(df_to_cluster_on, modelvar_cols_list, encode_cols_list, n_clusters, linkage_p, metric_p, clust_col_name, pp_exportdf_yn_name, mdl_exportdf_yn_name, scalar_obj_name, umap_obj_name):\n",
    "    # Preprocess\n",
    "    print(\"Beginning PreProcessing: \")\n",
    "    print(\"\")\n",
    "    pp_df = hierPreprocess(df_to_cluster_on)\n",
    "    print(\"Finished Preprocessing: \")\n",
    "    print(\"\")\n",
    "\n",
    "    # Perform Drop Duplicates\n",
    "    pp_df.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create a Copy of the dataframe to preserve original form\n",
    "    model_df = pp_df.copy()\n",
    "\n",
    "    # Encode if necessary\n",
    "    print(\"Beginning Encoding: \")\n",
    "    print(\"\")\n",
    "    if ((isinstance(encode_cols_list, list)) and (len(encode_cols_list) > 0)):\n",
    "        model_df = pd.get_dummies(model_df[ORIGIN_COLS], columns = encode_cols_list)\n",
    "    else:\n",
    "        model_df = model_df[ORIGIN_COLS]\n",
    "    print(\"Finished Encoding: \")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"Beginning Scaling: \")\n",
    "    print(\"\")\n",
    "    # Scale on 0-1\n",
    "    model_df = min_max_scale_export(model_df, modelvar_cols_list, scalar_obj_name)\n",
    "    print(\"Finished Scaling: \")\n",
    "    print(\"\")\n",
    "\n",
    "    # Coerce Data Types\n",
    "    print(\"Beginning Data Type Coercion: \")\n",
    "    print(\"\")\n",
    "    model_df = coerceDataTypes_small(model_df)\n",
    "    print(\"Finished Data Type Coercion: \")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Coerce Data Types\n",
    "    #print(\"Beginning UMAP Transformation: \")\n",
    "    #print(\"\")\n",
    "    #umap_df = umap_export(model_df, modelvar_cols_list, 20, umap_obj_name)\n",
    "    umap_df = pd.DataFrame()\n",
    "    #print(\"Finished UMAP Transformation: \")\n",
    "    #print(\"\")\n",
    "    \n",
    "    # Start the stopwatch\n",
    "    start_time = time.time()\n",
    "    # Model\n",
    "    print(\"Beginning Modeling: \")\n",
    "    print(\"\")\n",
    "    hier_model = AgglomerativeClustering(n_clusters = n_clusters, linkage = linkage_p, metric = metric_p).fit(model_df)\n",
    "    print(\"Finished Modeling: \")\n",
    "    print(\"\")\n",
    "    # Stop the stopwatch\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "    print(\"Calculating Silhouette Score: \")\n",
    "    print(\"\")\n",
    "    # Retain silhouette_score\n",
    "    silhouetteScore = silhouette_score(model_df, hier_model.labels_) \n",
    "    print(\"Done Calculating Silhouette Score: \")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Getting Clusters In Column Form: \")\n",
    "    print(\"\")\n",
    "    # Get Labels in List Form\n",
    "    clusters_list_form = (hier_model.labels_).tolist()\n",
    "    print(\"Done Getting Clusters In Column Form: \")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Appending Column Values: \")\n",
    "    print(\"\")\n",
    "    # Append\n",
    "    for df in [pp_df, model_df, umap_df]:\n",
    "        df[clust_col_name] = clusters_list_form\n",
    "        df[\"silhouette_score\"] = silhouetteScore\n",
    "    print(\"Done Appending Column Values: \")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Exporting CSVs if requested: \")\n",
    "    print(\"\")\n",
    "    # Export if needed\n",
    "    if pp_exportdf_yn_name[0] is True:\n",
    "        pp_df.to_csv(pp_exportdf_yn_name[1])\n",
    "    if mdl_exportdf_yn_name[0] is True:\n",
    "        model_df.to_csv(mdl_exportdf_yn_name[1])\n",
    "    print(\"Done Exporting CSVs if requested: \")\n",
    "    print(\"\")\n",
    "    \n",
    "    return df_to_cluster_on, pp_df, model_df, umap_df, clusters_list_form, silhouetteScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "jun_output = performHierClustering(jun_data_df, ML_COLS_L3, ENCODE_COLS, 14, linkage_p = \"complete\", metric_p = \"euclidean\", clust_col_name = \"clust\", \n",
    "                                   pp_exportdf_yn_name = [True, \"jun_hclust14_10p_pp_output.csv\"], mdl_exportdf_yn_name = [True, \"jun_hclust14_10p_mdl_output.csv\"], \n",
    "                                   scalar_obj_name = \"jun_hclust14_scalar.pkl\", umap_obj_name = \"jun_hclust14_umap.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b95a10-5e87-4b72-8b4d-f6a55196679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation, Debug, Troubleshoot\n",
    "#model_output[1] # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276b208-4043-4a03-be88-25c144c71dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b067bd1-1dc1-49e6-8069-a689dc7dc0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(model_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45558a-3955-4f01-a9be-119527a5b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Count Distribution\n",
    "#counts = Counter(model_output[1])\n",
    "#print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a816d8-1351-44fa-bbad-c1bf38b211fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_count_list = sum(1 for item in model_output[1] if item is None or (isinstance(item, float) and np.isnan(item)))\n",
    "#print(f\"Number of missing values: {missing_count_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79a31a-075b-4297-8fa5-1978cc752020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a7572-ea1f-4c74-ac48-8cd58c33e71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
