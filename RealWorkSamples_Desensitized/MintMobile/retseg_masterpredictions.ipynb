{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector as snow\n",
    "from snowflake.connector import pandas_tools\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List to Hold DFs\n",
    "MASTER_PRED_LIST = []\n",
    "# Names of Snowflake Table to Upload to\n",
    "SF_TBL_NAME = \"DESENSITIZED\"\n",
    "# Import Model to Use for Predictions\n",
    "PREFIX_PATH = \"C:/Users/davidl/OneDrive - ULTRA MOBILE/Desktop/dli_code/RetSeg/models/decisiontree_jun_mdl_114/\"\n",
    "MDL_NAME = \"jun_hclust13_10p_mdl.pkl\"\n",
    "SCALAR_NAME = \"jun_hclust13_scalar.pkl\"\n",
    "clust_mdl_pkl = PREFIX_PATH + MDL_NAME\n",
    "with open(clust_mdl_pkl, 'rb') as f:\n",
    "    clust_mdl = pickle.load(f)\n",
    "scalar_mdl_pkl = PREFIX_PATH + \"/scalar/\" + SCALAR_NAME\n",
    "#with open(scalar_mdl_pkl, 'rb') as f:\n",
    "#    scalar_mdl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS & FUNCTIONS Section\n",
    "\n",
    "# CONSTANTS\n",
    "ORIGIN_COLS = ['ACTIVATING_SALE_GROUP_NAME_grouped',\n",
    "'PROMO_GROUPED',\n",
    "'GSMA_OPERATING_SYSTEM_grouped',\n",
    "'ZERO_USAGE_LAST_30D_FLAG',\n",
    "'SUB_AUTO_RENEWAL_FLAG',\n",
    "'SUB_CREDIT_CARD_FLAG',\n",
    "'PLAN_CYCLE_NUM_grouped',\n",
    "'FAILED_PAYMENT_GROUPED',\n",
    "'MEMBER_OF_ACTIVE_FAMILY_FLAG',\n",
    "#'ACS_HP_PROP', \n",
    "#'ACS_NOT_HP_ASIAN_ALONE_PROP',\n",
    "#'ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP',\n",
    "#'ACS_NOT_HP_WHITE_ALONE_PROP',\n",
    "#'ACS_PROP_WORKERS_OVER_16',\n",
    "#'ACS_AGE_MEDIAN',\n",
    "#'ACS_INCOME_MEDIAN',\n",
    "#'ACS_APPROX_COMMUTE_MEDIAN',\n",
    "#'NO_SCHOOL_PROP',\n",
    "#'ANY_DEGREE_PROP',\n",
    "#'SINGLE_MOM_PROP',\n",
    "#'NEVER_MARRIED_PROP',\n",
    "#'HH_WO_INT_ACCESS_PROP',\n",
    "#'OCCUP_HOUS_UNIT_WO_CAR_PROP',\n",
    "'UPGRADE_DOWNGRADE_DATA_FLAG', \n",
    "'UPGRADE_DOWNGRADE_DURATION_FLAG',\n",
    "'SUB_ESIM_FLAG',\n",
    "'PORTIN_FLAG',\n",
    "#'HAD_ISSUES_PORTING_IN',\n",
    "'EVER_LOGGED_INTO_APP_FLAG',\n",
    "'YEAR_RELEASED_grouped',\n",
    "'LTE_BAND_71',\n",
    "#'contacted_care_last7d', \n",
    "'contacted_care_last30d',\n",
    "'SERVICE_ISSUE_NOTE_FLAG', \n",
    "#'SIM_REPLACEMENT_NOTE_FLAG',\n",
    "#'PAYMENT_NOTE_FLAG',\n",
    "'TENURE_MONTHS',\n",
    "'EXPECTED_CLV_PS',\n",
    "'PROMO_FLAG',\n",
    "'PORTIN_ISSUE_DESC',\n",
    "'L1_CLUST'\n",
    "]\n",
    "\n",
    "# All Columns\n",
    "ML_COLS_L3 = ['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm',\n",
    "    'ACTIVATING_SALE_GROUP_NAME_grouped_National Retail', \n",
    "    'PROMO_GROUPED_Deflation', \n",
    "    #'PROMO_GROUPED_Device bundle', \n",
    "    'PROMO_GROUPED_No Promo',\n",
    "    'GSMA_OPERATING_SYSTEM_grouped_iOS',\n",
    "    'PLAN_CYCLE_NUM_grouped', \n",
    "    'FAILED_PAYMENT_GROUPED',\n",
    "    'MEMBER_OF_ACTIVE_FAMILY_FLAG', \n",
    "    #'ACS_HP_PROP',\n",
    "    #'ACS_NOT_HP_ASIAN_ALONE_PROP',\n",
    "    #'ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP',\n",
    "    #'ACS_NOT_HP_WHITE_ALONE_PROP', \n",
    "    #'ACS_PROP_WORKERS_OVER_16',\n",
    "    #'ACS_AGE_MEDIAN', \n",
    "    #'ACS_INCOME_MEDIAN', \n",
    "    #'ACS_APPROX_COMMUTE_MEDIAN', \n",
    "    #'NO_SCHOOL_PROP',\n",
    "    #'ANY_DEGREE_PROP',\n",
    "    #'SINGLE_MOM_PROP',\n",
    "    #'NEVER_MARRIED_PROP',\n",
    "    #'HH_WO_INT_ACCESS_PROP',\n",
    "    #'OCCUP_HOUS_UNIT_WO_CAR_PROP',\n",
    "    'UPGRADE_DOWNGRADE_DATA_FLAG', \n",
    "    'UPGRADE_DOWNGRADE_DURATION_FLAG',\n",
    "    'SUB_ESIM_FLAG', \n",
    "    #'HAD_ISSUES_PORTING_IN', \n",
    "    'EVER_LOGGED_INTO_APP_FLAG',\n",
    "    'LTE_BAND_71',\n",
    "    #'contacted_care_last7d', \n",
    "    'contacted_care_last30d',\n",
    "    'SERVICE_ISSUE_NOTE_FLAG',\n",
    "    #'SIM_REPLACEMENT_NOTE_FLAG', \n",
    "    #'PAYMENT_NOTE_FLAG',\n",
    "    'EXPECTED_CLV_PS',\n",
    "    'PORTIN_ISSUE_DESC_NoIssues',\n",
    "    'PORTIN_ISSUE_DESC_NonPortin',\n",
    "    'L1_CLUST_0',\n",
    "    #'L1_CLUST_1',\n",
    "    'L1_CLUST_2',\n",
    "    'L1_CLUST_3',\n",
    "    'L1_CLUST_4',\n",
    "    'L1_CLUST_5',\n",
    "    'L1_CLUST_6'\n",
    "]\n",
    "\n",
    "# Columns to Encode\n",
    "ENCODE_COLS = ['ACTIVATING_SALE_GROUP_NAME_grouped', 'PROMO_GROUPED', 'GSMA_OPERATING_SYSTEM_grouped', 'PORTIN_ISSUE_DESC', 'L1_CLUST']\n",
    "\n",
    "# Columns to Stratify On\n",
    "STRATIFY_COLS = ['BLK_GRP', 'PROMO_FLAG', 'PROMO_GROUPED', 'LTE_BAND_71', 'ACTIVATING_SALE_GROUP_NAME']\n",
    "\n",
    "# FUNCTIONS\n",
    "def categorize_sale_group(column):\n",
    "    if 'Direct EComm' in str(column):\n",
    "        return 'Direct EComm'\n",
    "    if 'National Retail' in str(column):\n",
    "        return 'National Retail'\n",
    "    if 'Campus SIMs' in str(column):\n",
    "        return 'Campus SIMs'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def categorize_device_year(column):\n",
    "    try:\n",
    "        # Attempt to convert the column to an integer\n",
    "        column = int(float(column))\n",
    "    except (ValueError, TypeError):\n",
    "        # If conversion fails, set column to 0\n",
    "        column = 0\n",
    "\n",
    "    # Now perform the categorization\n",
    "    if column == 2024:\n",
    "        return 2024\n",
    "    elif column == 2023:\n",
    "        return 2023\n",
    "    elif column == 2022:\n",
    "        return 2022\n",
    "    elif column == 2021:\n",
    "        return 2021\n",
    "    elif column == 2020:\n",
    "        return 2020\n",
    "    elif column == 0:  # accounting for majority of devices being Galaxy A53 5G\n",
    "        return 2022\n",
    "    else:\n",
    "        return 2019\n",
    "\n",
    "def categorize_os(column):\n",
    "    if 'Android' in str(column):\n",
    "        return 'Android'\n",
    "    if 'KaiOS' in str(column):\n",
    "        return 'Other'\n",
    "    if 'iOS' in str(column):\n",
    "        return 'iOS'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "def categorize_plan_cycle_num(column):\n",
    "    if column <= 3:\n",
    "        return column\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Fit/Transforms a Scalar object on provided df, and exports a scalar object\n",
    "def min_max_scale_export(df, columns_to_scale, scalar_path):\n",
    "    \"\"\"\n",
    "    Applies Min-Max scaling to specified columns of a DataFrame to a range between 0 and 1, \n",
    "    ensuring all values are non-negative.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - columns_to_scale: List of column names to apply Min-Max Scaling.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the specified columns scaled to the range between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Adjust feature_range to (0, 1) for scaling between 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Apply MinMaxScaler to the specified columns\n",
    "    df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "    \n",
    "    # Save the scaler to a file using pickle\n",
    "    with open(scalar_path, 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    \n",
    "    return df[columns_to_scale]\n",
    "\n",
    "# Applies a provided scalar object to the df\n",
    "def min_max_scale_import(df, columns_to_scale, scalar_path):\n",
    "    \"\"\"\n",
    "    Loads a saved scaler and applies it to specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - columns_to_scale: List of column names to apply the saved scaler.\n",
    "    - scaler_path: Path to the saved scaler object.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the specified columns scaled using the saved scaler.\n",
    "    \"\"\"\n",
    "    # Load the scaler from the file using pickle\n",
    "    with open(scalar_path, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    \n",
    "    # Apply the loaded scaler to the specified columns\n",
    "    df[columns_to_scale] = scaler.transform(df[columns_to_scale])\n",
    "    \n",
    "    return df[columns_to_scale]\n",
    "\n",
    "# Define Pre-processing Steps\n",
    "def hierPreprocess(df):\n",
    "    ## Impute ACS Statistic Measures where NA\n",
    "    # Age\n",
    "    df['ACS_AGE_MEDIAN'] = df['TOTAL_POP_MEDIAN_AGE'].fillna(df['ACS_APPROX_AGE_MEDIAN'])\n",
    "    df['ACS_AGE_MEDIAN'] = df['ACS_AGE_MEDIAN'].fillna(df['ACS_AGE_MEDIAN'].median())\n",
    "    # Income\n",
    "    df['ACS_INCOME_MEDIAN'] = df['ACS_APPROX_INCOME_MEDIAN'].fillna(df['ACS_APPROX_INCOME_MEDIAN'].median())\n",
    "    # Commute\n",
    "    df['ACS_APPROX_COMMUTE_MEDIAN'] = df['ACS_APPROX_COMMUTE_MEDIAN'].fillna(df['ACS_APPROX_COMMUTE_MEDIAN'].median())\n",
    "    # Ethnicities\n",
    "    df['ACS_HP_PROP'] = df['ACS_HP_PROP'].fillna(df['ACS_HP_PROP'].median())\n",
    "    df['ACS_NOT_HP_ASIAN_ALONE_PROP'] = df['ACS_NOT_HP_ASIAN_ALONE_PROP'].fillna(df['ACS_NOT_HP_ASIAN_ALONE_PROP'].median())\n",
    "    df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'] = df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].fillna(df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].median())\n",
    "    df['ACS_NOT_HP_WHITE_ALONE_PROP'] = df['ACS_NOT_HP_WHITE_ALONE_PROP'].fillna(df['ACS_NOT_HP_WHITE_ALONE_PROP'].median())\n",
    "    # Proportion of Workers\n",
    "    df['ACS_PROP_WORKERS_OVER_16'] = df['ACS_PROP_WORKERS_OVER_16'].fillna(df['ACS_PROP_WORKERS_OVER_16'].median())\n",
    "\n",
    "    # Use Categorization/Bucketing Functions\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped'] = df['ACTIVATING_SALE_GROUP_NAME'].map(categorize_sale_group)\n",
    "    df['YEAR_RELEASED_grouped'] = df['YEAR_RELEASED'].map(categorize_device_year)\n",
    "    df['GSMA_OPERATING_SYSTEM_grouped'] = df['GSMA_OPERATING_SYSTEM'].map(categorize_os)\n",
    "    df['PLAN_CYCLE_NUM_grouped'] = df['PLAN_CYCLE_NUM'].map(categorize_plan_cycle_num)\n",
    "    df['contacted_care_last7d'] = np.where(df['CNT_NOTES_LAST_7D']>0,1,0)\n",
    "    df['contacted_care_last30d'] = np.where(df['CNT_NOTES_LAST_30D']>0,1,0)\n",
    "    \n",
    "    # Function to determine PORTIN_ISSUE_DESC\n",
    "    def determine_issue_desc(row):\n",
    "        if row['PORTIN_FLAG'] == 1 and row['HAD_ISSUES_PORTING_IN'] == 1:\n",
    "            return 'HadIssues'\n",
    "        elif row['PORTIN_FLAG'] == 1 and row['HAD_ISSUES_PORTING_IN'] == 0:\n",
    "            return 'NoIssues'\n",
    "        elif row['PORTIN_FLAG'] == 0:\n",
    "            return 'NonPortin'\n",
    "        else:\n",
    "            return 'UnKnown'  # In case there are other combinations\n",
    "    # Apply the function to create the new column\n",
    "    df['PORTIN_ISSUE_DESC'] = df.apply(determine_issue_desc, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def coerceDataTypes_small(df):\n",
    "    print(\"Memory Used Before Coercing: \")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    \n",
    "    #df['VOLTE_USAGE_LAST_7D']=df['VOLTE_USAGE_LAST_7D'].astype('int8')\n",
    "    df['PROMO_GROUPED_Deflation']=df['PROMO_GROUPED_Deflation'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail']=df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail'].astype('int8')\n",
    "    #df['ACS_INCOME_MEDIAN']=df['ACS_INCOME_MEDIAN'].astype('float16')\n",
    "    df['PLAN_CYCLE_NUM_grouped']=df['PLAN_CYCLE_NUM_grouped'].astype('int8')\n",
    "    df['FAILED_PAYMENT_GROUPED']=df['FAILED_PAYMENT_GROUPED'].astype('int8')  \n",
    "    df['EXPECTED_CLV_PS']=df['EXPECTED_CLV_PS'].astype('float16')   \n",
    "    df['MEMBER_OF_ACTIVE_FAMILY_FLAG']=df['MEMBER_OF_ACTIVE_FAMILY_FLAG'].astype('int8')\n",
    "    #df['ACS_HP_PROP']=df['ACS_HP_PROP'].astype('float16')\n",
    "    #df['ACS_NOT_HP_ASIAN_ALONE_PROP']=df['ACS_NOT_HP_ASIAN_ALONE_PROP'].astype('float16')\n",
    "    #df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP']=df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].astype('float16')\n",
    "    #df['ACS_NOT_HP_WHITE_ALONE_PROP']=df['ACS_NOT_HP_WHITE_ALONE_PROP'].astype('float16')\n",
    "    #df['ACS_PROP_WORKERS_OVER_16']=df['ACS_PROP_WORKERS_OVER_16'].astype('float16')\n",
    "    #df['ACS_AGE_MEDIAN']=df['ACS_AGE_MEDIAN'].astype('float16')\n",
    "    #df['ACS_APPROX_COMMUTE_MEDIAN']=df['ACS_APPROX_COMMUTE_MEDIAN'].astype('float16')\n",
    "    df['SUB_ESIM_FLAG']=df['SUB_ESIM_FLAG'].astype('int8')\n",
    "    #df['HAD_ISSUES_PORTING_IN']=df['HAD_ISSUES_PORTING_IN'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NoIssues']=df['PORTIN_ISSUE_DESC_NoIssues'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NonPortin']=df['PORTIN_ISSUE_DESC_NonPortin'].astype('int8')\n",
    "    df['EVER_LOGGED_INTO_APP_FLAG']=df['EVER_LOGGED_INTO_APP_FLAG'].astype('int8')\n",
    "    df['LTE_BAND_71']=df['LTE_BAND_71'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DATA_FLAG']=df['UPGRADE_DOWNGRADE_DATA_FLAG'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DURATION_FLAG']=df['UPGRADE_DOWNGRADE_DURATION_FLAG'].astype('int8')\n",
    "    #df['contacted_care_last7d']=df['contacted_care_last7d'].astype('int8')\n",
    "    df['contacted_care_last30d']=df['contacted_care_last30d'].astype('int8')\n",
    "    df['SERVICE_ISSUE_NOTE_FLAG']=df['SERVICE_ISSUE_NOTE_FLAG'].astype('int8')\n",
    "    #df['SIM_REPLACEMENT_NOTE_FLAG']=df['SIM_REPLACEMENT_NOTE_FLAG'].astype('int8')\n",
    "    #df['PAYMENT_NOTE_FLAG']=df['PAYMENT_NOTE_FLAG'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm']=df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm'].astype('int8')\n",
    "    #df['PROMO_GROUPED_Device bundle']=df['PROMO_GROUPED_Device bundle'].astype('int8')\n",
    "    df['PROMO_GROUPED_No Promo']=df['PROMO_GROUPED_No Promo'].astype('int8')\n",
    "    df['GSMA_OPERATING_SYSTEM_grouped_iOS']=df['GSMA_OPERATING_SYSTEM_grouped_iOS'].astype('int8')\n",
    "    #df['NO_SCHOOL_PROP']=df['NO_SCHOOL_PROP'].astype('float16')\n",
    "    #df['ANY_DEGREE_PROP']=df['ANY_DEGREE_PROP'].astype('float16')\n",
    "    #df['SINGLE_MOM_PROP']=df['SINGLE_MOM_PROP'].astype('float16')\n",
    "    #df['NEVER_MARRIED_PROP']=df['NEVER_MARRIED_PROP'].astype('float16')\n",
    "    #df['HH_WO_INT_ACCESS_PROP']=df['HH_WO_INT_ACCESS_PROP'].astype('float16')\n",
    "    #df['OCCUP_HOUS_UNIT_WO_CAR_PROP']=df['OCCUP_HOUS_UNIT_WO_CAR_PROP'].astype('float16')\n",
    "    df['L1_CLUST_0']=df['L1_CLUST_0'].astype(pd.Int8Dtype())\n",
    "    #df['L1_CLUST_1']=df['L1_CLUST_1'].astype('int8')\n",
    "    df['L1_CLUST_2']=df['L1_CLUST_2'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_3']=df['L1_CLUST_3'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_4']=df['L1_CLUST_4'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_5']=df['L1_CLUST_5'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_6']=df['L1_CLUST_6'].astype(pd.Int8Dtype())\n",
    "\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"Memory Used After Coercing\")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    print(\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def coerceDataTypes_med(df):\n",
    "    print(\"Memory Used Before Coercing: \")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    \n",
    "    #df['VOLTE_USAGE_LAST_7D']=df['VOLTE_USAGE_LAST_7D'].astype('int8')\n",
    "    df['PROMO_GROUPED_Deflation']=df['PROMO_GROUPED_Deflation'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail']=df['ACTIVATING_SALE_GROUP_NAME_grouped_National Retail'].astype('int8')\n",
    "    #df['ACS_INCOME_MEDIAN']=df['ACS_INCOME_MEDIAN'].astype('float32')\n",
    "    df['PLAN_CYCLE_NUM_grouped']=df['PLAN_CYCLE_NUM_grouped'].astype('int8')\n",
    "    df['FAILED_PAYMENT_GROUPED']=df['FAILED_PAYMENT_GROUPED'].astype('int8') \n",
    "    df['EXPECTED_CLV_PS']=df['EXPECTED_CLV_PS'].astype('float32')  \n",
    "    df['MEMBER_OF_ACTIVE_FAMILY_FLAG']=df['MEMBER_OF_ACTIVE_FAMILY_FLAG'].astype('int8')\n",
    "    #df['ACS_HP_PROP']=df['ACS_HP_PROP'].astype('float32')\n",
    "    #df['ACS_NOT_HP_ASIAN_ALONE_PROP']=df['ACS_NOT_HP_ASIAN_ALONE_PROP'].astype('float32')\n",
    "    #df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP']=df['ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP'].astype('float32')\n",
    "    #df['ACS_NOT_HP_WHITE_ALONE_PROP']=df['ACS_NOT_HP_WHITE_ALONE_PROP'].astype('float32')\n",
    "    #df['ACS_PROP_WORKERS_OVER_16']=df['ACS_PROP_WORKERS_OVER_16'].astype('float32')\n",
    "    #df['ACS_AGE_MEDIAN']=df['ACS_AGE_MEDIAN'].astype('float32')\n",
    "    #df['ACS_APPROX_COMMUTE_MEDIAN']=df['ACS_APPROX_COMMUTE_MEDIAN'].astype('float32')\n",
    "    df['SUB_ESIM_FLAG']=df['SUB_ESIM_FLAG'].astype('int8')\n",
    "    #df['HAD_ISSUES_PORTING_IN']=df['HAD_ISSUES_PORTING_IN'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NoIssues']=df['PORTIN_ISSUE_DESC_NoIssues'].astype('int8')\n",
    "    df['PORTIN_ISSUE_DESC_NonPortin']=df['PORTIN_ISSUE_DESC_NonPortin'].astype('int8')\n",
    "    df['EVER_LOGGED_INTO_APP_FLAG']=df['EVER_LOGGED_INTO_APP_FLAG'].astype('int8')\n",
    "    df['LTE_BAND_71']=df['LTE_BAND_71'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DATA_FLAG']=df['UPGRADE_DOWNGRADE_DATA_FLAG'].astype('int8')\n",
    "    df['UPGRADE_DOWNGRADE_DURATION_FLAG']=df['UPGRADE_DOWNGRADE_DURATION_FLAG'].astype('int8')\n",
    "    #df['contacted_care_last7d']=df['contacted_care_last7d'].astype('int8')\n",
    "    df['contacted_care_last30d']=df['contacted_care_last30d'].astype('int8')\n",
    "    df['SERVICE_ISSUE_NOTE_FLAG']=df['SERVICE_ISSUE_NOTE_FLAG'].astype('int8')\n",
    "    #df['SIM_REPLACEMENT_NOTE_FLAG']=df['SIM_REPLACEMENT_NOTE_FLAG'].astype('int8')\n",
    "    #df['PAYMENT_NOTE_FLAG']=df['PAYMENT_NOTE_FLAG'].astype('int8')\n",
    "    df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm']=df['ACTIVATING_SALE_GROUP_NAME_grouped_Direct EComm'].astype('int8')\n",
    "    #df['PROMO_GROUPED_Device bundle']=df['PROMO_GROUPED_Device bundle'].astype('int8')\n",
    "    df['PROMO_GROUPED_No Promo']=df['PROMO_GROUPED_No Promo'].astype('int8')\n",
    "    df['GSMA_OPERATING_SYSTEM_grouped_iOS']=df['GSMA_OPERATING_SYSTEM_grouped_iOS'].astype('int8')\n",
    "    #df['NO_SCHOOL_PROP']=df['NO_SCHOOL_PROP'].astype('float32')\n",
    "    #df['ANY_DEGREE_PROP']=df['ANY_DEGREE_PROP'].astype('float32')\n",
    "    #df['SINGLE_MOM_PROP']=df['SINGLE_MOM_PROP'].astype('float32')\n",
    "    #df['NEVER_MARRIED_PROP']=df['NEVER_MARRIED_PROP'].astype('float32')\n",
    "    #df['HH_WO_INT_ACCESS_PROP']=df['HH_WO_INT_ACCESS_PROP'].astype('float32')\n",
    "    #df['OCCUP_HOUS_UNIT_WO_CAR_PROP']=df['OCCUP_HOUS_UNIT_WO_CAR_PROP'].astype('float32')\n",
    "    df['L1_CLUST_0']=df['L1_CLUST_0'].astype(pd.Int8Dtype())\n",
    "    #df['L1_CLUST_1']=df['L1_CLUST_1'].astype('int8')\n",
    "    df['L1_CLUST_2']=df['L1_CLUST_2'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_3']=df['L1_CLUST_3'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_4']=df['L1_CLUST_4'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_5']=df['L1_CLUST_5'].astype(pd.Int8Dtype())\n",
    "    df['L1_CLUST_6']=df['L1_CLUST_6'].astype(pd.Int8Dtype())\n",
    "\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"Memory Used After Coercing\")\n",
    "    print(df.memory_usage(deep=True).sum()/1000/1000/1000)\n",
    "    print(\"\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Snowflake\n",
    "def get_col_types(df):\n",
    " \n",
    "    '''\n",
    "        Automatically parse for columns and their implicit data type form. Used for Schema inheriting for snowflake. \n",
    "        If needed to be created manually, can also be created in \"A NUMERIC, B VARCHAR, C FLOAT, D DATETIME\" format\n",
    "        \n",
    "        args:\n",
    "            df: dataframe to evaluate\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    # get dtypes and convert to df\n",
    "    ct = df.dtypes.reset_index().rename(columns={0:'col'})\n",
    "    ct = ct.apply(lambda x: x.astype(str).str.upper()) # case matching as snowflake needs it in uppers\n",
    "        \n",
    "    # only considers objects at this point\n",
    "    # only considers objects and ints at this point\n",
    "\n",
    "    ct['col'] = np.where(ct['col']=='OBJECT', 'VARCHAR', ct['col'])\n",
    "    ct['col'] = np.where(ct['col'].str.contains('DATE'), 'DATETIME', ct['col'])\n",
    "    ct['col'] = np.where(ct['col'].str.contains('INT'), 'NUMERIC', ct['col'])\n",
    "    ct['col'] = np.where(ct['col'].str.contains('FLOAT'), 'FLOAT', ct['col'])\n",
    "    \n",
    "    # get the column dtype pair\n",
    "    l = []\n",
    "    for index, row in ct.iterrows():\n",
    "        l.append(row['index'] + ' ' + row['col'])\n",
    "    \n",
    "    string = ', '.join(l) # convert from list to a string object\n",
    "    \n",
    "    string = string.strip()\n",
    "    \n",
    "    return string\n",
    "\n",
    "def create_table(table, action, conn, col_type, df):\n",
    "    \n",
    "    '''\n",
    "        Function to create/replace and append to tables in Snowflake\n",
    "        \n",
    "        args:\n",
    "            table: name of the table to create/modify\n",
    "            action: whether do the initial create/replace or appending; key to control logic\n",
    "            col_type: string with column name associated dtype, each pair separated by a comma; comes from get_col_types() func\n",
    "            df: dataframe to load\n",
    "            \n",
    "        dependencies: function get_col_types(); helper function to get the col and dtypes to create a table\n",
    "    ''' \n",
    "   \n",
    "\n",
    "    # set up cursor\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    if action=='create_replace':\n",
    "        \n",
    "        conn.cursor().execute(\"USE SCHEMA SANDBOX.STRATEGY_FINANCE\")\n",
    "        \n",
    "        setup_sql = \"\"\" CREATE OR REPLACE TABLE \n",
    "            \"\"\" + table +\"\"\"(\"\"\" + col_type + \"\"\")\"\"\"\n",
    "        \n",
    "        print(setup_sql)\n",
    "    \n",
    "        # set up execute\n",
    "        cur.execute(setup_sql) \n",
    "\n",
    "        #prep to ensure proper case\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "        # write df to table\n",
    "        pandas_tools.write_pandas(conn, df, table.upper())\n",
    "        \n",
    "    elif action=='append':\n",
    "        \n",
    "        conn.cursor().execute(\"USE SCHEMA SANDBOX.STRATEGY_FINANCE\")\n",
    "        pandas_tools.write_pandas(conn, df, table.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection Details\n",
    "con = snow.connect(\n",
    "    user=\"DESENSITIZED\",\n",
    "    server=\"DESENSITIZED\",\n",
    "    database=\"DESENSITIZED\",\n",
    "    warehouse=\"DESENSITIZED\",\n",
    "    authenticator=\"externalbrowser\",\n",
    "    account=\"DESENSITIZED\"\n",
    ")\n",
    "\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predNewData(dt_pkl, pred_df, create_pred_file):\n",
    "    # IMPORTANT: Assumes data is already scaled\n",
    "\n",
    "    # If dt_pkl is a direct model object\n",
    "    if hasattr(dt_pkl, 'predict'):\n",
    "        loaded_model = dt_pkl\n",
    "    # If dt_pkl is the string name of the path to the pickled model\n",
    "    elif isinstance(dt_pkl, str):\n",
    "        with open(dt_pkl, 'rb') as f:\n",
    "            loaded_model = pickle.load(f)\n",
    "    else:\n",
    "        print(\"ERROR: MODEL OBJECT UNKNOWN\")\n",
    "        return 0\n",
    "\n",
    "    print(\"Predicting On Given DF..\")\n",
    "    \n",
    "    # Identify rows where 'L1_Clust' is null\n",
    "    null_mask = pred_df[['L1_CLUST_0', 'L1_CLUST_2', 'L1_CLUST_3', 'L1_CLUST_4', 'L1_CLUST_5', 'L1_CLUST_6']].isnull().any(axis=1)\n",
    "    \n",
    "    # Initialize the prediction column with NaN\n",
    "    pred_df['pred_clust'] = np.nan\n",
    "    \n",
    "    # Make predictions for rows where 'L1_Clust' is not null\n",
    "    non_null_df = pred_df.loc[~null_mask]\n",
    "    features = non_null_df.drop(columns=['pred_clust'])  # Drop the prediction column if it exists\n",
    "    pred_df.loc[~null_mask, 'pred_clust'] = loaded_model.predict(features)\n",
    "    \n",
    "    # Save the predicted DataFrame to a file if requested\n",
    "    if create_pred_file[0].upper() == \"Y\":\n",
    "        pred_df.to_csv(create_pred_file[1], index=False)\n",
    "    \n",
    "    return pred_df\n",
    "    \n",
    "    # Make predictions\n",
    "    #new_preds = loaded_model.predict(pred_df)\n",
    "    \n",
    "    # Add predictions to the DataFrame\n",
    "    #pred_df['pred_clust'] = new_preds\n",
    "\n",
    "    \n",
    "    \n",
    "    #return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Master Flow for Prediction After Data Import\n",
    "def runPredModelMaster(df, mdl):\n",
    "    # Handle NAs\n",
    "    # Set the option to display more rows\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    # Check the number of missing values in each column\n",
    "    na_counts = df.isna().sum()\n",
    "    # Sort the counts in descending order\n",
    "    sorted_na_counts = na_counts.sort_values(ascending=False)\n",
    "    # Display the result\n",
    "    print(sorted_na_counts)\n",
    "    \n",
    "    # Handle NAs appropriately\n",
    "    df['VOLTE_USAGE_LAST_7D'] = df['VOLTE_USAGE_LAST_7D'].fillna(0)\n",
    "    df['ZERO_USAGE_LAST_30D_FLAG'] = df['ZERO_USAGE_LAST_30D_FLAG'].fillna(0)\n",
    "    df['SERVICE_ISSUE_NOTE_FLAG'] = df['SERVICE_ISSUE_NOTE_FLAG'].fillna(0)\n",
    "    df['SIM_REPLACEMENT_NOTE_FLAG'] = df['SIM_REPLACEMENT_NOTE_FLAG'].fillna(0)\n",
    "    df['PAYMENT_NOTE_FLAG'] = df['PAYMENT_NOTE_FLAG'].fillna(0)\n",
    "    df['LTE_BAND_71'] = df['LTE_BAND_71'].fillna(1)\n",
    "    df['HAD_ISSUES_PORTING_IN'] = df['HAD_ISSUES_PORTING_IN'].fillna(0)\n",
    "    df['EXPECTED_CLV_PS'] = df['EXPECTED_CLV_PS'].fillna(0)\n",
    "    df['UPGRADE_DOWNGRADE_DATA_FLAG'] = df['UPGRADE_DOWNGRADE_DATA_FLAG'].fillna(0)\n",
    "    df['CNT_NOTES_LAST_30D'] = df['CNT_NOTES_LAST_30D'].fillna(0)\n",
    "    df['CNT_NOTES_LAST_7D'] = df['CNT_NOTES_LAST_7D'].fillna(0)\n",
    "    df['PLAN_CYCLE_NUM'] = df['PLAN_CYCLE_NUM'].fillna(1)\n",
    "    # THIS NEEDS TO BE REMOVED ON REAL RUN\n",
    "    #df['L1_CLUST'] = df['L1_CLUST'].fillna(6)\n",
    "    # THIS NEEDS TO BE REMOVED ON REAL RUN\n",
    "    \n",
    "    # Handle NAs\n",
    "    # Set the option to display more rows\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    # Check the number of missing values in each column\n",
    "    na_counts = df.isna().sum()\n",
    "    # Sort the counts in descending order\n",
    "    sorted_na_counts = na_counts.sort_values(ascending=False)\n",
    "    # Display the result\n",
    "    print(sorted_na_counts)\n",
    "    \n",
    "    \n",
    "    # Run Preprocessing\n",
    "    subsnap_df = hierPreprocess(df)\n",
    "\n",
    "    # Perform Drop Duplicates\n",
    "    subsnap_df.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Enforce the incoming L1 Clusters as Integers\n",
    "    # Coerce the column to int8, leaving NA values as NA\n",
    "    subsnap_df['L1_CLUST'] = subsnap_df['L1_CLUST'].astype(pd.Int8Dtype())\n",
    "    #subsnap_df['L1_CLUST'] = pd.to_numeric(subsnap_df['L1_CLUST'], errors='coerce').astype('int8')  \n",
    "\n",
    "    # Prep\n",
    "    model_df = subsnap_df[ORIGIN_COLS]\n",
    "    ##\n",
    "    model_df_dummies = pd.get_dummies(model_df[ORIGIN_COLS], columns = ENCODE_COLS)\n",
    "    # Identify the rows where L1_CLUST is NaN in the original DataFrame\n",
    "    na_rows = model_df['L1_CLUST'].isna()\n",
    "    # Replace 0s with NaNs in the dummy columns for rows where L1_CLUST was NaN\n",
    "    for col in model_df_dummies.columns:\n",
    "        if col.startswith('L1_CLUST_'):\n",
    "            model_df_dummies.loc[na_rows, col] = np.nan\n",
    "    ##\n",
    "    \n",
    "    print(model_df_dummies.columns.values)\n",
    "    \n",
    "    model_df_dummies = min_max_scale_import(model_df_dummies, ML_COLS_L3, scalar_mdl_pkl)\n",
    "    model_df_dummies = coerceDataTypes_small(model_df_dummies)\n",
    "\n",
    "    # Reorder Columns\n",
    "    model_df_dummies = model_df_dummies[ML_COLS_L3]\n",
    "\n",
    "    # Run June Predictive Model, get predicted cluster column\n",
    "    subsnap_df[\"pred_clust\"] = predNewData(dt_pkl = mdl, pred_df = model_df_dummies, create_pred_file = [\"n\", \"pred_clust_test.csv\"])['pred_clust']\n",
    "\n",
    "    # Coerce for reducing format error liklihood\n",
    "    subsnap_df[\"pred_clust\"] = subsnap_df[\"pred_clust\"].astype(pd.Int8Dtype())\n",
    "    \n",
    "    return subsnap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_subs_sql = '''\n",
    "DESENSITIZED\n",
    "'''\n",
    "\n",
    "cur.execute(dec_subs_sql)\n",
    "dec_df = cur.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Incoming Data\n",
    "print(dec_df.shape)\n",
    "print(dec_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Model Prediction and Create Column\n",
    "dec_df_w_pred = runPredModelMaster(dec_df, clust_mdl)\n",
    "MASTER_PRED_LIST.append(dec_df_w_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conslidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_assertion_list = ['SUB_BILLING_ID', 'CHURNED_SUB_FLAG', 'SUB_AUTO_RENEWAL_FLAG',\n",
    "       'SUB_CREDIT_CARD_FLAG', 'SUB_ESIM_FLAG',\n",
    "       'ACTIVATING_SALE_GROUP_NAME', 'PORTIN_FLAG',\n",
    "       'HAD_ISSUES_PORTING_IN', 'GSMA_OPERATING_SYSTEM', 'YEAR_RELEASED',\n",
    "       'LTE_BAND_71', 'PLAN_CYCLE_NUM', 'PROMO_FLAG', 'PROMO_GROUPED',\n",
    "       'MEMBER_OF_ACTIVE_FAMILY_FLAG', 'EVER_LOGGED_INTO_APP_FLAG',\n",
    "       'UPGRADE_DOWNGRADE_DURATION_FLAG', 'UPGRADE_DOWNGRADE_DATA_FLAG',\n",
    "       'TENURE_MONTHS', 'EXPECTED_CLV_PS', 'FAILED_PAYMENT_GROUPED',\n",
    "       'COVERAGE_CLASS_4G', 'VOLTE_USAGE_LAST_7D',\n",
    "       'ZERO_USAGE_LAST_30D_FLAG', 'CNT_NOTES_LAST_30D',\n",
    "       'CNT_NOTES_LAST_7D', 'SERVICE_ISSUE_NOTE_FLAG',\n",
    "       'SIM_REPLACEMENT_NOTE_FLAG', 'PAYMENT_NOTE_FLAG',\n",
    "       'TOTAL_POP_MEDIAN_AGE', 'ACS_HP_PROP',\n",
    "       'ACS_NOT_HP_ASIAN_ALONE_PROP',\n",
    "       'ACS_NOT_HP_AFRICAN_AMERICAN_ALONE_PROP',\n",
    "       'ACS_NOT_HP_WHITE_ALONE_PROP', 'ACS_NOT_HP_OTHER_POP',\n",
    "       'ACS_PROP_WORKERS_OVER_16', 'ACS_APPROX_AGE_MEDIAN',\n",
    "       'ACS_APPROX_INCOME_MEDIAN', 'ACS_APPROX_COMMUTE_MEDIAN',\n",
    "       'NO_SCHOOL_PROP', 'ANY_DEGREE_PROP', 'SINGLE_MOM_PROP',\n",
    "       'NEVER_MARRIED_PROP', 'HH_WO_INT_ACCESS_PROP',\n",
    "       'OCCUP_HOUS_UNIT_WO_CAR_PROP', 'L1_CLUST', 'ACS_AGE_MEDIAN',\n",
    "       'ACS_INCOME_MEDIAN', 'ACTIVATING_SALE_GROUP_NAME_grouped',\n",
    "       'YEAR_RELEASED_grouped', 'GSMA_OPERATING_SYSTEM_grouped',\n",
    "       'PLAN_CYCLE_NUM_grouped', 'contacted_care_last7d',\n",
    "       'contacted_care_last30d', 'PORTIN_ISSUE_DESC', 'SNAPSHOT_DATE','pred_clust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks\n",
    "# Check Length\n",
    "print(len(MASTER_PRED_LIST))\n",
    "# Assert Columns are the same in all the DFs\n",
    "for df in MASTER_PRED_LIST:\n",
    "    if set(df.columns.values) != set(columns_assertion_list):\n",
    "        print(\"WARNING: UNMATCHED COLUMNS FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and Keep Only Sub_Billing_ID, Snapshot_date, Cluster\n",
    "concat_df = pd.concat(MASTER_PRED_LIST, axis=0, ignore_index=True)\n",
    "concat_short_df = concat_df[[\"SUB_BILLING_ID\", \"SNAPSHOT_DATE\", \"pred_clust\"]]\n",
    "concat_short_df.columns = concat_short_df.columns.str.upper()\n",
    "# Add Current Datetime for Tracking When Uploaded\n",
    "PD_CURR_DT = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "concat_short_df[\"UPDATE_DT\"] = PD_CURR_DT\n",
    "concat_short_df[\"UPDATE_DT\"] = concat_short_df[\"UPDATE_DT\"].astype('datetime64[ns]').dt.strftime('%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "concat_short_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write\n",
    "snowflake_syntax_enforcement = get_col_types(concat_short_df)\n",
    "create_table(SF_TBL_NAME, 'create_replace',con, snowflake_syntax_enforcement, concat_short_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
